from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import sys  # Required for real-time printing

# ðŸ”¹ Step 1: Manually Login using Access Token
HUGGINGFACE_TOKEN = "your_huggingface_token_here"  # Replace with your token
login(HUGGINGFACE_TOKEN)

# ðŸ”¹ Step 2: Load the Model and Tokenizer (on CPU)
MODEL_NAME = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="cpu")

# ðŸ”¹ Step 3: Hardcoded Schema
SCHEMA = """
Tables:
1. employees (id INT, name TEXT, age INT, department TEXT, salary INT)
2. departments (id INT, department_name TEXT, location TEXT)

Relations:
- employees.department is a foreign key referring to departments.id
"""

# ðŸ”¹ Step 4: Interactive Loop
print("\nðŸ”¹ AI is ready! Ask questions about the schema below.")
print("ðŸ›‘ Type 'exit' to quit.\n")

while True:
    question = input("ðŸ’¬ Ask your question: ")
    
    if question.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Exiting... Goodbye!")
        break

    # ðŸ”¹ Step 5: Format Prompt with Schema
    input_text = f"Schema:\n{SCHEMA}\n\nQuestion:\n{question}\n\nAnswer:"
    input_ids = tokenizer(input_text, return_tensors="pt")

    # ðŸ”¹ Step 6: Generate Response **Token-by-Token**
    print("\nðŸ’¬ AI Response:\n", end="", flush=True)

    with torch.no_grad():
        for output in model.generate(**input_ids, max_new_tokens=100, do_sample=True, output_scores=True):
            response = tokenizer.decode(output, skip_special_tokens=True)
            for token in response:
                sys.stdout.write(token)
                sys.stdout.flush()
                time.sleep(0.05)  # Adjust typing speed

    print("\n")  # New line after complete response

