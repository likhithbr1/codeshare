from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import sys  # For real-time output

# ðŸ”¹ Step 1: Login to Hugging Face
HUGGINGFACE_TOKEN = "your_huggingface_token_here"  # Replace with your token
login(HUGGINGFACE_TOKEN)

# ðŸ”¹ Step 2: Load Model and Tokenizer
MODEL_NAME = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="cpu")

# ðŸ”¹ Step 3: Hardcoded Schema
SCHEMA = """
Tables:
1. employees (id INT, name TEXT, age INT, department TEXT, salary INT)
2. departments (id INT, department_name TEXT, location TEXT)

Relations:
- employees.department is a foreign key referring to departments.id
"""

# ðŸ”¹ Step 4: Interactive Chat Loop
print("\nðŸ”¹ AI is ready! Ask questions about the schema below.")
print("ðŸ›‘ Type 'exit' to quit.\n")

while True:
    question = input("ðŸ’¬ Ask your question: ")
    
    if question.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Exiting... Goodbye!")
        break

    # ðŸ”¹ Step 5: Force Model to Output SQL Query Only
    input_text = f"""Schema:
{SCHEMA}

Question: {question}

Provide only the SQL query as the answer, without explanation.
SQL Query:"""
    
    input_ids = tokenizer(input_text, return_tensors="pt")

    # ðŸ”¹ Step 6: Generate Response
    with torch.no_grad():
        outputs = model.generate(**input_ids, max_new_tokens=100)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ðŸ”¹ Step 7: Print SQL Query Word-by-Word
    print("\nðŸ’¬ SQL Query:\n", end="", flush=True)
    for word in response.split():
        sys.stdout.write(word + " ")
        sys.stdout.flush()
        time.sleep(0.1)  # Adjust typing speed
    print("\n")  # New line after complete response

