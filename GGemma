from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import sys  # Required for real-time printing

# ðŸ”¹ Step 1: Manually Login using Access Token
HUGGINGFACE_TOKEN = "your_huggingface_token_here"  # Replace with your token
login(HUGGINGFACE_TOKEN)

# ðŸ”¹ Step 2: Load the Model and Tokenizer (on CPU)
MODEL_NAME = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="cpu")

# ðŸ”¹ Step 3: Hardcoded Schema
SCHEMA = """
Tables:
1. employees (id INT, name TEXT, age INT, department TEXT, salary INT)
2. departments (id INT, department_name TEXT, location TEXT)

Relations:
- employees.department is a foreign key referring to departments.id
"""

# ðŸ”¹ Step 4: Interactive Loop
print("\nðŸ”¹ AI is ready! Ask questions about the schema below.")
print("ðŸ›‘ Type 'exit' to quit.\n")

while True:
    question = input("ðŸ’¬ Ask your question: ")
    
    if question.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Exiting... Goodbye!")
        break

    # ðŸ”¹ Step 5: Format Prompt with Schema
    input_text = f"Schema:\n{SCHEMA}\n\nQuestion:\n{question}\n\nAnswer:"
    input_ids = tokenizer(input_text, return_tensors="pt")

    # ðŸ”¹ Step 6: Generate Response
    with torch.no_grad():
        outputs = model.generate(**input_ids, max_new_tokens=100)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ðŸ”¹ Step 7: Print Response Word by Word
    print("\nðŸ’¬ AI Response:\n", end="", flush=True)
    for word in response.split():
        sys.stdout.write(word + " ")  # Print each word with a space
        sys.stdout.flush()  # Ensure it appears immediately
        time.sleep(0.1)  # Add delay for effect
    print("\n")  # New line after complete response

