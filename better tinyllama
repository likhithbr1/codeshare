import os
import sys
from typing import Optional

# 1) If you installed the standard open-source LangChain:
# from langchain.sql_database import SQLDatabase
# from langchain.chains import create_sql_query_chain
# from langchain.agents.agent_types import AgentType
# from langchain.agents import create_sql_agent

# 2) If you have "langchain_community" version, do:
from langchain_community.utilities import SQLDatabase
from langchain.chains import create_sql_query_chain
from langchain_community.agent_toolkits import create_sql_agent
from langchain.agents.agent_types import AgentType

from sqlalchemy import create_engine
import langchain
langchain.debug = True

##############################
# TINYLLAMA LOAD
##############################
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

def load_tinyllama_langchain_llm():
    print("‚è≥ Loading TinyLlama model (HuggingFace style)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    print("‚úÖ Model loaded!")

    hf_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
    )
    # Convert to a LangChain LLM
    llm = HuggingFacePipeline(pipeline=hf_pipe)
    return llm


##############################
# PARTIAL SCHEMA SELECTION
##############################
def pick_tables(question: str, all_tables: list) -> list:
    """Naive approach: pick tables whose names appear in the question."""
    question_lower = question.lower()
    relevant = [t for t in all_tables if t.lower() in question_lower]
    # Fallback if no match
    return relevant or all_tables[:3]


##############################
# MAIN DEMO
##############################
def main():
    # 1) Load local TinyLlama model as an LLM
    llm = load_tinyllama_langchain_llm()

    # 2) MySQL credentials
    user = "root"
    password = "MY_SECRET"
    host = "localhost"
    port = 3306
    database = "my_database"

    db_uri = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

    print("\nüîπTinyLlama Chat w/ MySQL using LangChain.\n")
    print("Type 'exit' or 'quit' to stop.\n")

    while True:
        question = input("User Question: ")
        if question.strip().lower() in ["exit", "quit"]:
            break

        # 3) (Optional) PARTIAL REFLECTION
        #    Let's do a quick reflection first to get table names:
        wide_db = SQLDatabase.from_uri(db_uri, include_tables=None)
        all_table_names = wide_db.get_table_names()

        # 4) Choose which tables might be relevant
        relevant_tables = pick_tables(question, all_table_names)
        print(f"[*] Using these tables: {relevant_tables}")

        # 5) Create a DB object that only reflects columns for relevant tables
        filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)

        # 6) We'll do the simpler approach: create_sql_query_chain
        chain = create_sql_query_chain(llm, filtered_db, verbose=True)

        # 7) Run the chain
        try:
            result = chain.run(question)
            print(f"\nAssistant Answer:\n{result}\n")
        except Exception as e:
            print(f"\n‚ùå Error while generating/executing query: {e}")

    print("üëã Exiting. Goodbye!")

if __name__ == "__main__":
    main()
