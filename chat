from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch
import argparse
import os

# Set CPU thread count
torch.set_num_threads(os.cpu_count())

# Load Model and Tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(
    'gaussalgo/T5-LM-Large-text2sql-spider',
    torch_dtype=torch.float16,
    device_map="cpu",
)
tokenizer = AutoTokenizer.from_pretrained('gaussalgo/T5-LM-Large-text2sql-spider')

def generate_sql(question, schema):
    input_text = f"Question: {question} Schema: {schema}"
    print("Processing query...")

    model_inputs = tokenizer(input_text, return_tensors="pt")

    outputs = model.generate(
        **model_inputs,
        max_length=128,
        num_beams=1,  # Greedy search
        early_stopping=True
    )

    sql_query = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    return sql_query

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate SQL from natural language')
    parser.add_argument('--question', type=str, help='The natural language question')
    parser.add_argument('--schema_file', type=str, help='File containing the database schema')

    args = parser.parse_args()

    if args.question and args.schema_file:
        with open(args.schema_file, 'r') as f:
            schema = f.read()

        sql = generate_sql(args.question, schema)
        print("\nGenerated SQL Query:")
        print(sql)
    else:
        print("Please provide a question and schema_file.")
