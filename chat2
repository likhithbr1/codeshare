from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# Load Model and Tokenizer Once
print("Loading model... (This will happen only once)")
model = AutoModelForSeq2SeqLM.from_pretrained(
    'gaussalgo/T5-LM-Large-text2sql-spider',
    torch_dtype=torch.float16,
    device_map="cpu",
)
tokenizer = AutoTokenizer.from_pretrained('gaussalgo/T5-LM-Large-text2sql-spider')

def generate_sql(question, schema):
    input_text = f"Question: {question} Schema: {schema}"
    print("Processing query...")

    model_inputs = tokenizer(input_text, return_tensors="pt")

    outputs = model.generate(
        **model_inputs,
        max_length=128,
        num_beams=1,  # Greedy search
        early_stopping=False  # Disabled as it's not needed for greedy search
    )

    sql_query = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    return sql_query

def main():
    # Load schema once
    schema_file = 'schema.txt'
    try:
        with open(schema_file, 'r') as f:
            schema = f.read()
        print("Schema loaded.")
    except FileNotFoundError:
        print(f"Error: {schema_file} not found.")
        return

    while True:
        question = input("Enter your natural language question (type 'exit' to quit): ")
        if question.lower() == 'exit':
            print("Exiting application.")
            break

        sql = generate_sql(question, schema)
        print("\nGenerated SQL Query:")
        print(sql)

if __name__ == "__main__":
    main()
