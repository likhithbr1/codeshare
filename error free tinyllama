import os
import sys
from typing import Optional

# If you have "langchain_community" installed:
# pip install -U langchain-community
from langchain_community.utilities import SQLDatabase
from langchain_community.llms import HuggingFacePipeline
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
from langchain_community.agent_toolkits import create_sql_agent
from langchain.chains import create_sql_query_chain

from sqlalchemy import create_engine
import langchain
langchain.debug = True

##############################
# TINYLLAMA LOAD
##############################
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"


def load_tinyllama_langchain_llm():
    print("‚è≥ Loading TinyLlama model (HuggingFace style)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    print("‚úÖ Model loaded!")

    hf_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
    )
    # Convert to a LangChain(-community) LLM
    llm = HuggingFacePipeline(pipeline=hf_pipe)
    return llm

##############################
# PARTIAL SCHEMA SELECTION
##############################
def pick_tables(question: str, all_tables: list) -> list:
    """Naive approach: pick tables whose names appear in the question."""
    question_lower = question.lower()
    relevant = [t for t in all_tables if t.lower() in question_lower]
    # Fallback if no match
    return relevant or all_tables[:3]


def main():
    # 1) Load local TinyLlama model as an LLM
    llm = load_tinyllama_langchain_llm()

    # 2) MySQL credentials
    user = "root"
    password = "MY_SECRET"
    host = "localhost"
    port = 3306
    database = "my_database"

    db_uri = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"

    print("\nüîπTinyLlama Chat w/ MySQL using LangChain.\n")
    print("Type 'exit' or 'quit' to stop.\n")

    # 3) Connect to the DB so we can do partial reflection
    wide_db = SQLDatabase.from_uri(db_uri, include_tables=None)
    all_table_names = wide_db.get_usable_table_names()
    print(f"[DEBUG] All Table Names: {all_table_names}")

    # Prepare a "tool" to actually execute queries
    execute_query = QuerySQLDataBaseTool(db=wide_db)

    # 4) We'll build the chain each time the user asks a question
    while True:
        question = input("User Question: ")
        if question.strip().lower() in ["exit", "quit"]:
            break

        # pick relevant tables
        relevant_tables = pick_tables(question, all_table_names)
        print(f"[DEBUG] Using these tables: {relevant_tables}")

        # reflect only those relevant tables
        filtered_db = SQLDatabase.from_uri(db_uri, include_tables=relevant_tables)

        # produce SQL from user question:
        write_query = create_sql_query_chain(llm, filtered_db)

        # 5) Create a small "answer" prompt to ensure the final text is short and direct
        #    For example, it might say: "Just output the final SQL or final answer"
        short_prompt = PromptTemplate.from_template(
            "Given the SQL query and results, provide only a concise final answer:\n\nQuery: {query}\nResults: {result}\n\nAnswer:"
        )

        # We chain it together:
        #   a) generate "query" using 'write_query' (the chain)
        #   b) run that "query" with 'execute_query' => "result"
        #   c) pass both to the final LLM prompt => short text
        answer = short_prompt | llm | StrOutputParser()

        chain = (
            RunnablePassthrough.assign(query=write_query).assign(
                result=itemgetter("query") | execute_query
            )
            | answer
        )

        # 6) Now we run the chain with the user question
        #    The chain expects { "question": question }, used by write_query internally
        try:
            response = chain.invoke({"question": question})
            print(f"\nAssistant's Final Answer:\n{response}\n")
        except Exception as e:
            print(f"\n‚ùå Error while generating/executing query: {e}")

    print("üëã Exiting. Goodbye!")


if __name__ == "__main__":
    main()
