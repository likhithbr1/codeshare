import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Model Name (Google's Gemma-2B)
MODEL_NAME = "google/gemma-2b"

# Load Model and Tokenizer
def load_model():
    print("‚è≥ Loading Gemma-2B model... This may take a minute.")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.bfloat16,  # Use bfloat16 (Gemma does not support float16)
        device_map="auto",           # Automatically selects the best available device
    )
    
    print("‚úÖ Model loaded successfully!")
    return tokenizer, model

# Generate SQL Query
def generate_sql(question, schema, tokenizer, model, max_tokens=128):
    input_text = f"Generate an SQL query.\n\nSchema:\n{schema}\n\nQuestion:\n{question}\n\nSQL:"

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=max_tokens,
        do_sample=True,       # Enables sampling for more diverse responses
        temperature=0.7,      # Controls randomness (lower = more deterministic)
        top_p=0.9,           # Nucleus sampling
        return_full_text=False,
    )

    sql_query = pipe(input_text, num_return_sequences=1)[0]["generated_text"]
    return sql_query.strip()

def main():
    # Load model once
    tokenizer, model = load_model()

    # Load schema from file
    schema_file = 'schema.txt'
    try:
        with open(schema_file, 'r') as f:
            schema = f.read().strip()
        print("‚úÖ Schema loaded.")
    except FileNotFoundError:
        print(f"‚ùå Error: {schema_file} not found.")
        return

    print("\nüîπ Text-to-SQL Generator is ready! Type your question below.")
    print("üõë Type 'exit' to quit the application.\n")

    while True:
        question = input("üí¨ Enter your natural language question: ")
        if question.lower() in ["exit", "quit"]:
            print("üëã Exiting application. Goodbye!")
            break

        print("\n‚è≥ Generating SQL Query...\n")
        sql_query = generate_sql(question, schema, tokenizer, model)
        print(f"‚úÖ Generated SQL Query:\n{sql_query}\n")

if __name__ == "__main__":
    main()


from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# üîπ Replace 'your_huggingface_token_here' with your actual token
HUGGINGFACE_TOKEN = "your_huggingface_token_here"

# ‚úÖ Login manually
login(HUGGINGFACE_TOKEN)

# Model details
MODEL_NAME = "google/gemma-2b"

# üîπ Load tokenizer and model
print("‚è≥ Loading Gemma-2B model... This may take a minute.")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")

# Sample prompt
prompt = "Translate the following English text to French: 'Hello, how are you?'"

# Tokenize input
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# Generate response
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

# Decode and print result
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nüí¨ AI Response:", response)
