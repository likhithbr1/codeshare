import pymysql
from sqlalchemy import create_engine
from langchain_community.utilities import SQLDatabase
import langchain
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# Enable debugging for better visibility
langchain.debug = True

# 🔹 Step 1: MySQL Connection Details
MYSQL_USER = "your_username"  # Replace with your MySQL username
MYSQL_PASSWORD = "your_password"  # Replace with your MySQL password
MYSQL_HOST = "localhost"  # Change if needed
MYSQL_DATABASE = "your_database"  # Replace with your database name

# 🔹 Step 2: Connect to MySQL Database using Connection Pooling
engine = create_engine(
    f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}/{MYSQL_DATABASE}",
    pool_size=10,  # Allows up to 10 connections
    max_overflow=5  # Allows extra temporary connections
)
db = SQLDatabase(engine)

# 🔹 Step 3: Fetch Table Metadata Dynamically
def get_table_schema():
    """Fetches table names and their column details from the database."""
    table_names = db.get_usable_table_names()  # Get all table names
    schema_info = ""

    for table in table_names:
        schema_info += f"\nTable: {table}\n"
        schema_info += db.run(f"DESCRIBE {table};")  # Get column details

    return schema_info

SCHEMA = get_table_schema()

# 🔹 Step 4: Hugging Face LLM Setup (Gemma-2B)
HUGGINGFACE_TOKEN = "your_token_here"  # Replace with your token
login(HUGGINGFACE_TOKEN)

MODEL_NAME = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="cpu")

print("\n🔹 AI is ready! Ask questions about the schema below.")
print("🛑 Type 'exit' to quit.\n")

# 🔹 Step 5: Query Extraction Function
def extract_sql(response: str):
    """Extracts the SQL query from the LLM response using regex."""
    match = re.search(r"SELECT .* FROM .*", response, re.DOTALL | re.IGNORECASE)
    return match.group(0) if match else "Invalid SQL Query"

# 🔹 Step 6: Chat Loop with Optimized SQL Generation
while True:
    question = input("💬 Ask your question: ")
    
    if question.lower() in ["exit", "quit"]:
        print("👋 Exiting... Goodbye!")
        break

    # 🔹 Step 7: Format Prompt with Dynamic Schema
    input_text = f"Schema:\n{SCHEMA}\n\nQuestion:\n{question}\n\nAnswer: Only return the SQL query, no explanations."
    input_ids = tokenizer(input_text, return_tensors="pt")

    # 🔹 Step 8: Generate SQL Query
    with torch.no_grad():
        outputs = model.generate(**input_ids, max_new_tokens=100)

    raw_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sql_query = extract_sql(raw_response)  # Extract clean SQL

    print("\n💬 Generated SQL Query:\n", sql_query, "\n")

    # 🔹 Step 9: Execute SQL Query (with Error Handling)
    try:
        result = db.run(sql_query)
        print("📊 Query Results:\n", result, "\n")
    except Exception as e:
        print("❌ SQL Query Error:", e, "\n")
