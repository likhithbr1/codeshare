import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Model Name (TinyLlama-7B)
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Check Hugging Face for latest versions

# Load Tokenizer and Model
def load_model():
    print("‚è≥ Loading TinyLlama model... This may take a minute.")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,  # Use float16 for efficiency
        device_map="auto",         # Automatically choose the best available device
    )
    print("‚úÖ Model loaded successfully!")
    return tokenizer, model

# Generate response
def generate_response(prompt, tokenizer, model, max_tokens=100):
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=max_tokens,
        do_sample=True,       # Enables sampling for more diverse responses
        temperature=0.7,      # Controls randomness (lower = more deterministic)
        top_p=0.9,           # Nucleus sampling
        return_full_text=False,
    )

    response = pipe(prompt, num_return_sequences=1)[0]["generated_text"]
    return response.strip()

if __name__ == "__main__":
    # Load model once
    tokenizer, model = load_model()

    print("\nüîπ TinyLlama Chatbot is ready! Type your message below.")
    print("üõë Type 'exit' to quit the application.\n")

    # Interactive loop
    while True:
        user_input = input("üí¨ Enter your prompt: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üëã Exiting. Goodbye!")
            break

        print("\n‚è≥ Generating response...\n")
        response = generate_response(user_input, tokenizer, model)
        print(f"‚úÖ Model Response:\n{response}\n")
